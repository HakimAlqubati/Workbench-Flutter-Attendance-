// =============================
// File: lib/features/face_liveness/controller/face_liveness_controller.dart
// =============================
import 'dart:async';
import 'dart:io';
import 'dart:math' as math;
import 'dart:typed_data';
import 'dart:ui' as ui;
import 'package:image/image.dart' as img;
import 'package:gallery_saver_plus/gallery_saver.dart';
import 'package:camera/camera.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:flutter/widgets.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:my_app/core/network_helper.dart';
import 'package:my_app/features/attendance/attendance_service.dart';

// 1) Ø§Ù„ØªÙ‚Ø· Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ø¹Ø±Ø¶Ù‡Ø§ ÙÙˆØ±Ù‹Ø§
import 'package:path_provider/path_provider.dart';
import 'package:path/path.dart' as path;

import '../constants.dart';
import '../services/network_service.dart';


class FaceLivenessController extends ChangeNotifier with WidgetsBindingObserver {

  final ValueNotifier<String?> bannerMessage = ValueNotifier<String?>(null);

  void showBanner(String msg) {
    bannerMessage.value = msg;
  }

  void clearBanner() {
    bannerMessage.value = null;
  }

  Future<String?> Function()? onRequireType;

  // ===== Dependencies / Services =====
  final LivenessNetworkService _net = LivenessNetworkService();
  int _warmUpFrames = 0;

  VoidCallback? onLivenessFailed;

  DateTime? _lastBlendTs;

  Map<String, dynamic>? _attendanceResult;
  Map<String, dynamic>? get attendanceResult => _attendanceResult;


  // ===== Camera =====
  CameraController? _controller;
  CameraController? get controller => _controller;
  CameraDescription? _frontCamera;
  List<CameraDescription> _allCams = const [];


  bool _useFront = false; // âœ… Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: false = Ø®Ù„ÙÙŠØ© (Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹)
  bool get isFrontCamera => _useFront;
  CameraDescription? _rearCamera;
  // ===== Detector =====
  FaceDetector? _detector;
  bool _isDetecting = false;

  // ===== State =====
  bool _cameraOpen = true;
  bool get cameraOpen => _cameraOpen;

  bool _faceDetected = false;
  bool get faceDetected => _faceDetected;

  double _ratioProgress = 0.0;
  double get ratioProgress => _ratioProgress;

  bool _centeredInOval = false;
  bool get centeredInOval => _centeredInOval;

  double _centerScore = 0.0; // 0..1
  double get centerScore => _centerScore;

  Offset? _centerOffsetPx;
  Offset? get centerOffsetPx => _centerOffsetPx;

  bool _captureEligible = false;
  bool get captureEligible => _captureEligible;

  void _setCaptureEligible(bool v) {
    if (_captureEligible == v) return;
    _captureEligible = v;
    debugPrint('[ELIGIBLE -> ${v ? 'YES' : 'NO'}]');
    notifyListeners();
  }

  set _setRatioProgress(double v) {
    _ratioProgress = v.clamp(0.0, 1.0);
    notifyListeners();
  }

  // Ù‚ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©
  int _frameCount = 0;
  int _detectCounter = 0;
  double? _brightnessLevel; // 0..255
  double? get brightnessLevel => _brightnessLevel;
  String? _brightnessStatus;
  String? get brightnessStatus => _brightnessStatus;

  // Ø§Ù„Ø¹Ø¯ Ø§Ù„ØªÙ†Ø§Ø²Ù„ÙŠ
  int? _countdown;
  int? get countdown => _countdown;
  bool get isCountdownActive => _countdown != null && _countdown! > 0;
  bool _isSnapshotting = false;

  // Ø§Ù„Ù†ØªØ§Ø¦Ø¬
  Map<String, dynamic>? _livenessResult;
  Map<String, dynamic>? get livenessResult => _livenessResult;

  Map<String, dynamic>? _faceRecognitionResult;
  Map<String, dynamic>? get faceRecognitionResult => _faceRecognitionResult;

  XFile? _capturedFile;
  XFile? get capturedFile => _capturedFile;

  // For painters
  Rect? _lastFaceRect;
  Rect? get lastFaceRect => _lastFaceRect;

  Size? _latestImageSize;
  Size? get latestImageSize => _latestImageSize;

  bool _insideOval = false;
  bool get insideOval => _insideOval;

  bool _readyForNextImage = true;
  bool _isStreaming = false;
  bool _streamLock = false;

  // Screensaver / inactivity
  Timer? _inactivityTimer;
  Timer? _inactivityTicker;
  int? _screensaverCountdown;
  int? get screensaverCountdown => _screensaverCountdown;

  bool _showScreensaver = false;
  bool get showScreensaver => _showScreensaver;

  // ===== Waiting state (Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± ÙÙˆÙ‚ Ø§Ù„ØµÙˆØ±Ø©) =====
  bool _waiting = false;
  bool get waiting => _waiting;

  int _captureSeq = 0;        // token Ù…ØªØ²Ø§ÙŠØ¯ Ù„ÙƒÙ„ Ù„Ù‚Ø·Ø©
  int? _activeCaptureSeq;     // token Ø§Ù„Ø­Ø§Ù„ÙŠ Ù‚ÙŠØ¯ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±

  String _waitMessage = '';
  String get waitMessage => _waitMessage;

  bool get livenessPending =>
      kEnableLiveness && _waiting && (_livenessResult == null);
  bool get recognitionPending =>
      kEnableFaceRecognition && _waiting && (_faceRecognitionResult == null);

  // Clock
  final List<String> _clockPositions = ['center', 'right', 'left'];
  int _clockPosIndex = 0;
  int get clockPosIndex => _clockPosIndex;

  Timer? _clockMoveTimer;
  bool _clockBlink = false;
  bool get clockBlink => _clockBlink;

  Timer? _clockBlinkTimer;
  DateTime _now = DateTime.now();
  DateTime get now => _now;
  Timer? _nowTicker;

  // Layout-dependent
  Size _screenSize = Size.zero;
  set screenSize(Size s) => _screenSize = s;

  // ==== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø¬Ù… ====
  final FaceSizeThresholds _sizeCfg;
  FaceLivenessController({FaceSizeThresholds? sizeCfg})
      : _sizeCfg = sizeCfg ?? FaceSizeThresholds.defaults;

  // Ø­Ø¬Ù… Ø§Ù„ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø³Ø¨ÙŠ (ÙŠÙØ­Ø¯Ù‘Ø« ÙƒÙ„ Ø¥Ø·Ø§Ø±)
  double? _sizeRaw;
  double? get sizeRawLive => _sizeRaw;

  // ==== ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…Ø³Ø§ÙØ© (ØªÙ‚Ø±ÙŠØ¨ÙŠ) ====
  static const double _kIdealDistanceCm = 22.0;

  double? get estDistanceCm {
    final s = _sizeRaw;
    if (s == null || s <= 0) return null;
    return _kIdealDistanceCm * (_sizeCfg.rawIdeal / s);
  }

  double? get deltaToIdealCm {
    final d = estDistanceCm;
    if (d == null) return null;
    return d - _kIdealDistanceCm;
  }

  double? get deltaToRangeCm {
    final s = _sizeRaw;
    if (s == null || s <= 0) return null;
    if (s >= _sizeCfg.rawMin && s <= _sizeCfg.rawMax) return 0.0;

    final dNow = estDistanceCm!;
    if (s < _sizeCfg.rawMin) {
      final dWant = _kIdealDistanceCm * (_sizeCfg.rawIdeal / _sizeCfg.rawMin);
      return dNow - dWant; // Ù…ÙˆØ¬Ø¨Ø© => Ø§Ù‚ØªØ±Ø¨
    }
    final dWant = _kIdealDistanceCm * (_sizeCfg.rawIdeal / _sizeCfg.rawMax);
    return dNow - dWant; // Ù…ÙˆØ¬Ø¨Ø© => Ø§Ø¨ØªØ¹Ø¯
  }

  bool get tooFar => _sizeRaw != null && _sizeRaw! < _sizeCfg.rawMin;
  bool get tooClose => _sizeRaw != null && _sizeRaw! > _sizeCfg.rawMax;

  double get fitPct => (_ratioProgress.clamp(0.0, 1.0)) * 100.0;

  // ===== Ø­Ù‚Ù„ Ù„Ù„Ø­Ø¶ÙˆØ± Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„ÙƒÙ„ Ù„Ù‚Ø·Ø© =====
  bool _postedAttendanceForThisCapture = false;

  // Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©: Ù„Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø¢Ø®Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
  String? _lastApiMessage;
  String? get lastApiMessage => _lastApiMessage;
  bool _lastApiOk = false;
  bool get lastApiOk => _lastApiOk;

  // ===== Lifecycle =====
  Future<void> init() async {
    WidgetsBinding.instance.addObserver(this);
    _initDetector();
    _resetInactivity();
    _nowTicker = Timer.periodic(const Duration(seconds: 1), (_) {
      if (_showScreensaver) {
        _now = DateTime.now();
        notifyListeners();
      }
    });
    await _initCamera();
  }

  Future<void> disposeAll() async {
    WidgetsBinding.instance.removeObserver(this);
    _stopEverything();
    await _disposeCamera();
    _detector?.close();
    _detector = null;
    super.dispose();
  }

  @override
  void didChangeAppLifecycleState(AppLifecycleState state) async {
    if (_controller == null) return;
    if (state == AppLifecycleState.inactive || state == AppLifecycleState.paused) {
      await _stopStreamSafely();
    } else if (state == AppLifecycleState.resumed) {
      if (!_showScreensaver) await _startStreamSafely();
    }
  }

  Future<void> _startDisplayAndResume({required int seq}) async {
    // Ø£Ø¨Ù‚Ù Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹Ø±ÙˆØ¶Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¯Ø© (Ø­ØªÙ‰ ÙŠÙ‚Ø±Ø£ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø§Ù†Ø±Ø§Øª)
    await Future.delayed(Duration(milliseconds: kDisplayImageMs));

    // Ù„Ø§ ØªØ±Ø¬Ø¹ Ø¥Ø°Ø§ ØªØºÙŠÙ‘Ø± Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ùˆ ØªØºÙŠÙ‘Ø± Ø§Ù„Ù€ token
    if (_showScreensaver) return;
    if (_activeCaptureSeq != seq) return;

    await _resumeLivePreview();
  }

  // Ø²Ø± Ø§Ù„ØªØ®Ø·ÙŠ Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
  Future<void> skipWaiting() async {
    if (!_waiting) return;
    final seq = _activeCaptureSeq;
    if (seq == null) return;
    _waiting = false;
    notifyListeners();
    await _startDisplayAndResume(seq: seq);
  }

  // ===== Init parts =====
  void _initDetector() {
    _detector = FaceDetector(
      options: FaceDetectorOptions(
        performanceMode: FaceDetectorMode.fast,
        enableContours: false,
        enableLandmarks: false,
        enableClassification: false,
        enableTracking: false,
      ),
    );
  }


  Future<void> _initCamera() async {
    // âœ… Ø§Ø¬Ù„Ø¨ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
    _allCams = await availableCameras();

    // âœ… Ø­Ø§Ø±Ø³: Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒØ§Ù…ÙŠØ±Ø§Øª
    if (_allCams.isEmpty) {
      debugPrint('âŒ No cameras available');
      return;
    }

    // âœ… Ø¹ÙŠÙ‘Ù† Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø®Ù„ÙÙŠØ© Ù…Ø¹ orElse ØºÙŠØ± Ù‚Ø§Ø¨Ù„ Ù„Ù€ null
    _frontCamera = _allCams.firstWhere(
          (c) => c.lensDirection == CameraLensDirection.front,
      orElse: () => _allCams.first, // non-null
    );
    _rearCamera = _allCams.firstWhere(
          (c) => c.lensDirection == CameraLensDirection.front,
      orElse: () => _allCams.first, // non-null
    );

    // âœ… Ø§Ø®ØªØ± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¯Ø§ÙØ¹Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù„Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ Ù…Ø¹ fallback Ù…Ù†Ø·Ù‚ÙŠ
    final CameraDescription camToUse =
    _useFront ? (_frontCamera ?? _rearCamera!) : (_rearCamera ?? _frontCamera!);

    // âœ… Ø£Ù†Ø´Ø¦ Ø§Ù„ÙƒÙˆÙ†ØªØ±ÙˆÙ„Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
    _controller = CameraController(
      camToUse,
      ResolutionPreset.medium,
      enableAudio: false,
      imageFormatGroup: ImageFormatGroup.nv21,
    );
    await _controller!.initialize();
    await _controller!.lockCaptureOrientation(DeviceOrientation.portraitUp);

    // âœ… Ø®Ø²Ù‘Ù† Ø­Ø¬Ù… Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø£Ø®ÙŠØ±
    _latestImageSize = Size(
      _controller!.value.previewSize?.width ?? 1280,
      _controller!.value.previewSize?.height ?? 720,
    );

    // âœ… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
    _controller!.addListener(() async {
      final v = _controller!.value;
      if (v.hasError) {
        await _stopStreamSafely();
        await _startStreamSafely();
      }
    });

    await _startStreamSafely();
    notifyListeners();
  }


  

  Future<void> _disposeCamera() async {
    try {
      if (_controller != null) {
        await _stopStreamSafely();
        await _controller!.dispose();
      }
    } catch (_) {}
    _controller = null;
  }

  // ===== Inactivity / screensaver =====
  void _resetInactivity() {
    _inactivityTimer?.cancel();
    _inactivityTicker?.cancel();
    _screensaverCountdown = kScreensaverSeconds;

    _inactivityTicker = Timer.periodic(const Duration(seconds: 1), (_) {
      if (!_showScreensaver) {
        if (_screensaverCountdown != null && _screensaverCountdown! > 0) {
          _screensaverCountdown = _screensaverCountdown! - 1;
          notifyListeners();
        }
      }
    });

    _inactivityTimer = Timer(Duration(seconds: kScreensaverSeconds), () async {
      _showScreensaver = true;
      _cameraOpen = false;
      notifyListeners();
      await _disposeCamera();
    });

    _screensaverCountdown = kScreensaverSeconds;
    notifyListeners();
  }

  void userActivity() {
    if (_showScreensaver) return;
    _resetInactivity();
  }

  Future<void> exitScreensaverAndReopen() async {
    _showScreensaver = false;
    _cameraOpen = true;
    _capturedFile = null;
    _livenessResult = null;
    _faceRecognitionResult = null;
    _attendanceResult = null;
    _stopCountdown(force: true);
    _isSnapshotting = false;
    _isDetecting = false;
    _readyForNextImage = true;
    _faceDetected = false;
    _ratioProgress = 0.0;
    _brightnessLevel = null;
    _brightnessStatus = null;
    _detectCounter = 0;
    _lastFaceRect = null;
    _insideOval = false;
    _sizeRaw = null;
    _setCaptureEligible(false);
    await _initCamera();
    _resetInactivity();
    notifyListeners();
  }

  // ===== Stream control =====
  Future<void> _startStreamSafely() async {
    if (_controller == null) return;
    if (_streamLock) return;
    if (_controller!.value.isStreamingImages || _isStreaming) return;
    if (!_controller!.value.isInitialized) return;
    try {
      _streamLock = true;
      await _controller!.startImageStream(_onNewCameraImage);
      _isStreaming = true;
    } catch (_) {
    } finally {
      _streamLock = false;
    }
  }

  Future<void> _stopStreamSafely() async {
    if (_controller == null) return;
    if (_streamLock) return;
    if (!_controller!.value.isStreamingImages && !_isStreaming) return;
    try {
      _streamLock = true;
      await _controller!.stopImageStream();
    } catch (_) {
    } finally {
      _isStreaming = false;
      _streamLock = false;
    }
  }

  Future<void> _resumeLivePreview() async {
    _resetInactivity();
    _capturedFile = null;
    _livenessResult = null;
    _faceRecognitionResult = null;
    _attendanceResult = null;
    _cameraOpen = true;
    _isSnapshotting = false;
    _readyForNextImage = true;
    _insideOval = false;
    _ratioProgress = 0.0;

    _warmUpFrames = 12; // â¬…ï¸ ØªØ¬Ø§Ù‡Ù„ Ø£ÙˆÙ„ 12 Ø¥Ø·Ø§Ø± Ø¨Ø¹Ø¯ Ø§Ù„Ø±Ø¬ÙˆØ¹

    notifyListeners();
    await Future.delayed(const Duration(milliseconds: 120));
    await _startStreamSafely();
  }

  // ===== Core vision loop =====
  Future<void> _onNewCameraImage(CameraImage image) async {
    if (_warmUpFrames > 0) {
      _warmUpFrames--;
      _readyForNextImage = true;
      return;
    }

    if (!_readyForNextImage || !_cameraOpen) return;
    _readyForNextImage = false;

    _latestImageSize = Size(image.width.toDouble(), image.height.toDouble());

    // Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙƒÙ„ N Ø¥Ø·Ø§Ø±
    _frameCount = (_frameCount + 1) % kBrightnessSampleEveryN;
    if (_frameCount == 0) {
      final luma = _estimateLuma(image);
      _brightnessLevel = luma;
      _brightnessStatus = _statusForLuma(luma);
      notifyListeners();
    }

    // ØªÙ‚Ù„ÙŠÙ„ ÙƒÙ„ÙØ© Ø§Ù„ÙƒØ´Ù
    _detectCounter = (_detectCounter + 1) % kDetectEveryN;
    if (_detectCounter != 0) {
      _readyForNextImage = true;
      return;
    }
    if (_isDetecting) {
      _readyForNextImage = true;
      return;
    }
    _isDetecting = true;

    try {
      final inputImage = _toInputImage(image);
      final faces = await _detector!.processImage(inputImage);

      // âœ… Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ¯ÙÙ‚: Ø§Ø³ØªØ®Ø¯Ù… if/else ÙˆÙ„ÙŠØ³ if + if + else
      if (faces.isEmpty) {
        _lastFaceRect = null;
        _insideOval = false;
        _sizeRaw = null;
        _updateFaceDetected(false);
        _collapseProgressFast(factor: 0.35);
        _stopCountdown();
      } else {
        // Ø§Ø®ØªØ± Ø£ÙƒØ¨Ø± ÙˆØ¬Ù‡
        final face = faces.reduce((a, b) =>
        (a.boundingBox.width * a.boundingBox.height) >
            (b.boundingBox.width * b.boundingBox.height)
            ? a
            : b);

        final rect = face.boundingBox;
        _lastFaceRect = rect;

        final rawW = image.width.toDouble();
        final rawH = image.height.toDouble();

        final bool isFront =
            _frontCamera?.lensDirection == CameraLensDirection.front;

        // Ø¯Ø§Ø®Ù„/Ù…Ø±ÙƒØ² Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
        _insideOval = _isFaceInsideOvalOnScreen(
          faceCenter: rect.center,
          imageRawSize: Size(rawW, rawH),
          screenSize: _screenSize,
          isFront: isFront,
        );

        _centeredInOval = _isFaceCenteredInOvalOnScreen(
          faceCenter: rect.center,
          imageRawSize: Size(rawW, rawH),
          screenSize: _screenSize,
          isFront: isFront,
        );

        final imgShort = math.min(rawW, rawH);
        final faceShort = math.min(rect.width, rect.height);
        final double sizeRaw = (imgShort == 0 ? 0.0 : faceShort / imgShort);
        _sizeRaw = sizeRaw;

        final double sizeFactor = _sizeScoreWindowed(sizeRaw);
        final double posFactor = _positionFactor(
          faceCenterRaw: rect.center,
          imageRawSize: Size(rawW, rawH),
          screenSize: _screenSize,
          isFront: isFront,
        );

        // ØªÙ‚Ø¯Ù‘Ù… Ø´Ø±ÙŠØ· Ø§Ù„Ù…Ù„Ø§Ø¡Ù…Ø©
        final inFrame = _insideOval || _centerScore >= 0.22; // ØªØ³Ø§Ù‡Ù„ Ø¨Ø³ÙŠØ·
        if (!inFrame || posFactor == 0.0) {
          _setRatioProgress = 0.0;
        } else {
          final double targetProgress =
          (sizeFactor * posFactor).clamp(0.0, 1.0);
          _blendProgress(targetProgress, smooth: 0.22);
        }

        // Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙˆØ¬Ù‡
        _updateFaceDetected(sizeRaw >= _sizeCfg.rawMin * 0.75);

        debugPrint('Hakim{$sizeRaw}');
        debugPrint('rawMin: ${_sizeCfg.rawMin.toStringAsFixed(3)}');
        debugPrint('rawMax: ${_sizeCfg.rawMax.toStringAsFixed(3)}');
        final bool goodLighting = _brightnessStatus == 'Good lighting âœ…' ||
            _brightnessStatus == 'Excellent lighting ğŸŒŸ';

        // âœ… Ø§Ù„Ø£Ù‡Ù„ÙŠØ© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ (ØªÙ…Ø±ÙƒØ² Ø§Ù„Ù…Ø±ÙƒØ² + Ø­Ø¬Ù… Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚)
        final bool eligible = _faceDetected &&
            _insideOval &&
            sizeRaw >= _sizeCfg.rawMin &&
            sizeRaw <= _sizeCfg.rawMax

            && goodLighting
        ;

        _setCaptureEligible(eligible);

        if (eligible) {
          _beginCountdown();
        } else {
          _stopCountdown();
        }
      }
    } catch (_) {
      // ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù„Ø­Ø¸ÙŠØ©
    } finally {
      _isDetecting = false;
      _readyForNextImage = true;
      notifyListeners();
      await Future.delayed(const Duration(milliseconds: 2));
    }
  }

  // ===== ØªÙ…Ø±ÙƒØ²/Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ (Ù…ÙØ±Ø®Ù‘ÙØµ) =====

  bool _isFaceCenteredInOvalOnScreen({
    required Offset faceCenter,
    required Size imageRawSize,
    required Size screenSize,
    required bool? isFront,
    double epsilonPct = 0.05,
  }) {
    if (screenSize == Size.zero) {
      _centerScore = 0.0;
      _centerOffsetPx = null;
      return false;
    }

    final srcW = imageRawSize.height; // portrait width
    final srcH = imageRawSize.width; // portrait height
    final scale = math.max(screenSize.width / srcW, screenSize.height / srcH);
    final dxPad = (screenSize.width - srcW * scale) / 2.0;
    final dyPad = (screenSize.height - srcH * scale) / 2.0;

    double cx = faceCenter.dx * scale + dxPad;
    final double cy = faceCenter.dy * scale + dyPad;

    if (isFront == true) {
      final midX = screenSize.width / 2;
      cx = 2 * midX - cx;
    }

    final ovalCx = screenSize.width * (0.5 + kOvalCxOffsetPct);
    final ovalCy = screenSize.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = (screenSize.width * kOvalRxPct);
    final ovalRy = (screenSize.height * kOvalRyPct);

    final offXpx = cx - ovalCx;
    final offYpx = cy - ovalCy;
    _centerOffsetPx = Offset(offXpx, offYpx);

    final dxn = ovalRx == 0 ? 0.0 : offXpx / ovalRx;
    final dyn = ovalRy == 0 ? 0.0 : offYpx / ovalRy;

    final rAllow = epsilonPct.clamp(0.02, 0.9);
    final r = math.sqrt(dxn * dxn + dyn * dyn);

    _centerScore = (1.0 - (r / rAllow)).clamp(0.0, 1.0);
    return r <= rAllow;
  }

  // Ø³Ù…Ø§Ø­ÙŠØ©: 15% Ø®Ø§Ø±Ø¬ Ø§Ù„Ø­Ø¯ Ù…Ù‚Ø¨ÙˆÙ„Ø©ØŒ Ø£Ùˆ 3 Ø²ÙˆØ§ÙŠØ§ Ù…Ù† 4 Ø¯Ø§Ø®Ù„
  static const double _kEdgeOverflowTol = 0.22; // 15% Ø®Ø§Ø±Ø¬ Ø§Ù„Ø­Ø¯
  static const double _kCornersNeeded = 4; // 3 Ø²ÙˆØ§ÙŠØ§ ÙƒÙØ§ÙŠØ©

  // Ø§Ø³ØªØ¨Ø¯Ù„ Ø¯Ø§Ù„Ø© _isFaceInsideOvalOnScreen Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ù‡Ø°Ù‡ (Ø¥Ø²Ø§Ù„Ø© return Ø§Ù„Ù…ÙƒØ±Ø± ØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ÙˆØµÙˆÙ„)
  bool _isFaceInsideOvalOnScreen({
    required Offset faceCenter,
    required Size imageRawSize,
    required Size screenSize,
    required bool? isFront,
  }) {
    if (screenSize == Size.zero) return false;

    final srcW = imageRawSize.height;
    final srcH = imageRawSize.width;
    final scale = math.max(screenSize.width / srcW, screenSize.height / srcH);
    final dx = (screenSize.width - srcW * scale) / 2.0;
    final dy = (screenSize.height - srcH * scale) / 2.0;

    final faceRect = _lastFaceRect;
    if (faceRect == null) return false;

    final corners = <Offset>[
      faceRect.topLeft,
      faceRect.topRight,
      faceRect.bottomLeft,
      faceRect.bottomRight,
    ];

    final ovalCx = screenSize.width * (0.5 + kOvalCxOffsetPct);
    final ovalCy = screenSize.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = (screenSize.width * kOvalRxPct);
    final ovalRy = (screenSize.height * kOvalRyPct);

    int insideCount = 0;
    for (var point in corners) {
      double cx = point.dx * scale + dx;
      double cy = point.dy * scale + dy;

      if (isFront == true) {
        final midX = screenSize.width / 2;
        cx = 2 * midX - cx;
      }

      final dxn = (cx - ovalCx) / ovalRx;
      final dyn = (cy - ovalCy) / ovalRy;
      final distance = dxn * dxn + dyn * dyn; // <= 1 Ø¯Ø§Ø®Ù„
      if (distance <= (1.0 + _kEdgeOverflowTol)) insideCount++;
    }

    // ØªØ­Ù‚Ù‘ÙÙ‚ Ø¥Ø¶Ø§ÙÙŠ: Ù…Ø±ÙƒØ² Ø§Ù„ÙˆØ¬Ù‡ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ù…Ø¹ Ø³Ù…Ø§Ø­ÙŠØ© Ù†ØµÙÙŠØ©
    final faceCxRaw = faceRect.center;
    double ccx = faceCxRaw.dx * scale + dx;
    double ccy = faceCxRaw.dy * scale + dy;
    if (isFront == true) {
      final midX = screenSize.width / 2;
      ccx = 2 * midX - ccx;
    }
    final cdxn = (ccx - ovalCx) / ovalRx;
    final cdyn = (ccy - ovalCy) / ovalRy;
    final centerInside =
        (cdxn * cdxn + cdyn * cdyn) <= (1.0 + _kEdgeOverflowTol * 0.5);

    return insideCount >= _kCornersNeeded && centerInside;
  }

  void _updateFaceDetected(bool detected) {
    if (detected == _faceDetected) return;
    _faceDetected = detected;
    if (!detected) {
      _ratioProgress = ui.lerpDouble(_ratioProgress, 0.0, 0.12)!;
    } else {
      if (!_isSnapshotting) {
        _livenessResult = null;
        _faceRecognitionResult = null;
        _attendanceResult = null;
        _capturedFile = null;
      }
      _resetInactivity();
    }
  }

  void _beginCountdown() {
    if (_isSnapshotting || !_cameraOpen) return;
    if (isCountdownActive) return;
    _countdown = kCountdownSeconds;

    userActivity();
    Timer.periodic(const Duration(seconds: 1), (Timer t) async {
      if (_showScreensaver) {
        t.cancel();
        return;
      }
      if (!_captureEligible) {
        _stopCountdown();
        t.cancel();
        return;
      }
      if (_isSnapshotting) {
        t.cancel();
        return;
      }

      if (_countdown != null && _countdown! > 0) {
        _countdown = _countdown! - 1;
        notifyListeners();
        if (_countdown == 0) {
          _countdown = 0;
          _isSnapshotting = true;
          notifyListeners();
          t.cancel();
          scheduleMicrotask(() async {
            await _handleLivenessCheck();
          });
        }
      }
    });
    notifyListeners();
  }
  /// Ù‚Øµ ØµÙˆØ±Ø© Ø¯Ø§Ø®Ù„ Ø¨ÙŠØ¶Ø§ÙˆÙŠ (Oval) Ø­Ø³Ø¨ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø´Ø§Ø´Ø©
  Future<File> cropToOval(File originalFile, Size screenSize, {double scale = 1.0}) async {
    final bytes = await originalFile.readAsBytes();
    final src = img.decodeImage(bytes)!;

    // Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ØµÙˆØ±Ø© Ù†ÙØ³Ù‡Ø§
    final ovalCx = src.width * (0.5 + kOvalCxOffsetPct);
    final ovalCy = src.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = src.width * kOvalRxPct;
    final ovalRy = src.height * kOvalRyPct;

    // Ø§Ù„Ù…Ø³ØªØ·ÙŠÙ„ Ø§Ù„Ù…Ø­ÙŠØ· Ø¨Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
    final left   = (ovalCx - ovalRx).clamp(0, src.width - 1).toInt();
    final top    = (ovalCy - ovalRy).clamp(0, src.height - 1).toInt();
    final right  = (ovalCx + ovalRx).clamp(0, src.width - 1).toInt();
    final bottom = (ovalCy + ovalRy).clamp(0, src.height - 1).toInt();

    // âœ… Ø§Ø¬Ø¹Ù„Ù‡Ø§ Ù…Ø±Ø¨Ø¹Ø©
    int side = math.min(right - left, bottom - top);

    // âœ… Ø·Ø¨Ù‘Ù‚ Ø§Ù„ØªÙƒØ¨ÙŠØ±/Ø§Ù„ØªØµØºÙŠØ±
    side = (side * scale).toInt().clamp(10, math.min(src.width, src.height));

    // âœ… Ù‚Øµ Ù…Ø±Ø¨Ø¹ Ù…ØªÙ…Ø±ÙƒØ²
    final squareLeft = (ovalCx - side / 2).clamp(0, src.width - side).toInt();
    final squareTop  = (ovalCy - side / 2).clamp(0, src.height - side).toInt();

    final croppedSquare = img.copyCrop(
      src,
      x: squareLeft,
      y: squareTop,
      width: side,
      height: side,
    );

    // Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
    final croppedPath = originalFile.path.replaceFirst('.jpg', '_square.jpg');
    final croppedFile = File(croppedPath)
      ..writeAsBytesSync(img.encodeJpg(croppedSquare, quality: 95));

    return croppedFile;
  }


  void _stopCountdown({bool force = false}) {
    if (_isSnapshotting && !force) return;
    _countdown = null;
    notifyListeners();
  }

  // ===== Capture & backend =====
  Future<void> _handleLivenessCheck() async {
    if (!_captureEligible) {
      _isSnapshotting = false;
      _readyForNextImage = true;
      _stopCountdown(force: true);
      await _startStreamSafely();
      return;
    }

    if (_controller == null || !_controller!.value.isInitialized) return;

    // token Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù„Ù‚Ø·Ø©
    final int seq = ++_captureSeq;
    _activeCaptureSeq = seq;

    _postedAttendanceForThisCapture = false;

    try {
      _readyForNextImage = false;
      _isDetecting = false;
      await _stopStreamSafely();
      await Future.delayed(const Duration(milliseconds: 80));

      if (!_captureEligible) {
        _isSnapshotting = false;
        _readyForNextImage = true;
        _stopCountdown(force: true);
        await _startStreamSafely();
        return;
      }

      // 1ï¸âƒ£ Ø§Ù„ØªÙ‚Ø· Ø§Ù„ØµÙˆØ±Ø©
      final XFile captured = await _controller!.takePicture();

      // 2ï¸âƒ£ Ø§Ù†Ø³Ø® Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
      final Directory dir = await getApplicationDocumentsDirectory();
      final Directory folder = Directory(path.join(dir.path, 'liveness_captures'));
      if (!await folder.exists()) {
        await folder.create(recursive: true);
      }
      final String filename = 'capture_${DateTime.now().millisecondsSinceEpoch}.jpg';
      final String savedPath = path.join(folder.path, filename);
      final File savedFile = await File(captured.path).copy(savedPath);

      // âœ‚ï¸ Ù‚Øµ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
      final File ovalFile = await cropToOval(savedFile, _screenSize,scale: kCropScale);

      // 3ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚ØµÙˆØµØ© ÙÙŠ Ø§Ù„Ù…Ø¹Ø±Ø¶ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
      // await GallerySaver.saveImage(ovalFile.path, albumName: 'LivenessCaptures');

      // âœ… Ø§Ø¹ØªÙ…Ø¯ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚ØµÙˆØµØ© ÙÙ‚Ø·
      _capturedFile = XFile(ovalFile.path);

      // (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø§Ø­ØªÙØ¸ Ø¨Ù†Ø³Ø®Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
      final String ovalSavedPath = path.join(folder.path, 'oval_${DateTime.now().millisecondsSinceEpoch}.jpg');
      await File(ovalFile.path).copy(ovalSavedPath);

      // Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· Ø§Ù„Ø­Ø§Ù„Ø©
      _lastFaceRect = null;
      _livenessResult = null;
      _faceRecognitionResult = null;
      _attendanceResult = null;
      _resetInactivity();

      _waiting = true;
      _waitMessage = '';
      notifyListeners();

      // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‡Ø§Ù… (liveness + recognition)
      final futures = <Future<void>>[];

      if (kEnableLiveness) {
        final connected = await NetworkHelper.checkAndToastConnection();
        if (!connected) {
          _livenessResult = {'error': 'Check Your Internet Connection'};
          _waiting = false;
          notifyListeners();
          return;
        }
        futures.add(
          _net.sendLiveness(ovalFile.path).then((liveJson) {
            if (_activeCaptureSeq != seq) return;
            _livenessResult = liveJson ?? {'error': 'Invalid response'};
            notifyListeners();
          }).catchError((e) {
            if (_activeCaptureSeq != seq) return;
            _livenessResult = {'error': e.toString()};
            notifyListeners();
          }),
        );
      }

      await Future.wait(futures);
      if (kEnableFaceRecognition  &&
          _livenessResult != null &&
          _livenessResult?['status'] == 'ok' &&
          _livenessResult?['result']?['liveness'] == true) {
        final connected = await NetworkHelper.checkAndToastConnection();
        if (!connected) return;

        futures.add(
          _net.sendFaceRecognition(ovalFile.path).then((recog) async {
            if (_activeCaptureSeq != seq) return;
            _faceRecognitionResult = recog ?? {'error': 'Invalid response'};
            notifyListeners();
          }).catchError((e) {
            if (_activeCaptureSeq != seq) return;
            _faceRecognitionResult = {'error': e.toString()};
            notifyListeners();
          }),
        );
      }

      // Soft timeout
      Future.delayed(Duration(milliseconds: kSoftTimeoutMs), () {
        if (_activeCaptureSeq == seq && _waiting) {
          _waitMessage = 'Taking longer than usualâ€¦';
          notifyListeners();
        }
      });

      await Future.wait(futures);

      await _maybeAutoPostAttendance();

      if (_activeCaptureSeq == seq) {
        _waiting = false;
        notifyListeners();
        await _startDisplayAndResume(seq: seq);
      }
    } catch (_) {
      _livenessResult ??= {'error': 'Error sending image to backend!'};
      _waiting = false;
      notifyListeners();
      await _startDisplayAndResume(seq: _activeCaptureSeq ?? ++_captureSeq);
    } finally {
      _isSnapshotting = false;
      _readyForNextImage = true;
    }
  }

  // ===== Helpers =====
  InputImage _toInputImage(CameraImage image) {
    final bytes = _concatenatePlanes(image.planes);
    final rotation = _rotationIntToImageRotation(
      _controller?.description.sensorOrientation ?? 0,
    );
    final inputFormat = (image.format.group == ImageFormatGroup.nv21)
        ? InputImageFormat.nv21
        : InputImageFormat.yuv420;

    return InputImage.fromBytes(
      bytes: bytes,
      metadata: InputImageMetadata(
        size: Size(image.width.toDouble(), image.height.toDouble()),
        rotation: rotation,
        format: inputFormat,
        bytesPerRow: image.planes.first.bytesPerRow,
      ),
    );
  }

  Uint8List _concatenatePlanes(List<Plane> planes) {
    final WriteBuffer allBytes = WriteBuffer();
    for (final Plane plane in planes) {
      allBytes.putUint8List(plane.bytes);
    }
    return allBytes.done().buffer.asUint8List();
  }

  InputImageRotation _rotationIntToImageRotation(int rotation) {
    switch (rotation) {
      case 0:
        return InputImageRotation.rotation0deg;
      case 90:
        return InputImageRotation.rotation90deg;
      case 180:
        return InputImageRotation.rotation180deg;
      case 270:
        return InputImageRotation.rotation270deg;
      default:
        return InputImageRotation.rotation0deg;
    }
  }

  void _stopEverything() {
    _inactivityTimer?.cancel();
    _inactivityTicker?.cancel();
    _clockMoveTimer?.cancel();
    _clockBlinkTimer?.cancel();
    _nowTicker?.cancel();
  }

  void _decayRatio() {
    const decay = 0.85;
    _setRatioProgress = _ratioProgress * decay;
    if (_ratioProgress < 0.005) {
      _setRatioProgress = 0.0;
    }
  }

  void _updateRatio(Rect faceRect, Size imageSize) {
    final imgShort = math.min(imageSize.width, imageSize.height);
    final faceShort = math.min(faceRect.width, faceRect.height);
    final raw = (faceShort / imgShort).clamp(0.0, 1.0);

    const target = 0.22;
    final targetProgress = (raw / target).clamp(0.0, 1.0);

    const smooth = 0.22;
    _setRatioProgress =
        _ratioProgress + (targetProgress - _ratioProgress) * smooth;

    notifyListeners();
  }

  double _sizeScore(Rect faceRect, Size imageSize, {double target = 0.22}) {
    final imgShort = math.min(imageSize.width, imageSize.height);
    final faceShort = math.min(faceRect.width, faceRect.height);
    final raw = (faceShort / (imgShort == 0 ? 1 : imgShort)).clamp(0.0, 1.0);
    return (raw / target).clamp(0.0, 1.0);
  }

  double _smoothstep(double a, double b, double x) {
    if (a == b) return x >= b ? 1.0 : 0.0;
    final t = ((x - a) / (b - a)).clamp(0.0, 1.0);
    return t * t * (3 - 2 * t);
  }

  double _sizeScoreWindowed(double raw) {
    double sstep(double a, double b, double x) {
      if (a == b) return x >= b ? 1.0 : 0.0;
      final t = ((x - a) / (b - a)).clamp(0.0, 1.0);
      return t * t * (3 - 2 * t);
    }

    final up = sstep(_sizeCfg.rawMin, _sizeCfg.rawIdeal, raw);
    final down = 1.0 - sstep(_sizeCfg.rawIdeal, _sizeCfg.rawMax, raw);
    final peak = math.min(up, down);
    return peak.clamp(0.0, 1.0);
  }

  double _positionFactor({
    required Offset faceCenterRaw,
    required Size imageRawSize,
    required Size screenSize,
    required bool isFront,
  }) {
    if (screenSize == Size.zero) return 0.0;

    final srcW = imageRawSize.height; // portrait width
    final srcH = imageRawSize.width; // portrait height
    final scale = math.max(screenSize.width / srcW, screenSize.height / srcH);
    final dxPad = (screenSize.width - srcW * scale) / 2.0;
    final dyPad = (screenSize.height - srcH * scale) / 2.0;

    double cx = faceCenterRaw.dx * scale + dxPad;
    final double cy = faceCenterRaw.dy * scale + dyPad;

    if (isFront) {
      final midX = screenSize.width / 2;
      cx = 2 * midX - cx;
    }

    final ovalCx = screenSize.width * (0.5 + kOvalCxOffsetPct);
    final ovalCy = screenSize.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = (screenSize.width * kOvalRxPct);
    final ovalRy = (screenSize.height * kOvalRyPct);

    final dxn = (cx - ovalCx) / (ovalRx == 0 ? 1 : ovalRx);
    final dyn = (cy - ovalCy) / (ovalRy == 0 ? 1 : ovalRy);
    final r = math.sqrt(dxn * dxn + dyn * dyn);

    if (r >= 1.0) return 0.0;

    const p = 1.4;
    return math.pow((1.0 - r), p).toDouble().clamp(0.0, 1.0);
  }

  // Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø³Ø®ØªÙŠÙ† Ø§Ù„ØªÙŠ Ù„Ø¯ÙŠÙƒ:
  double? _lumaEma;
  double _estimateLuma(CameraImage image, {bool smooth = true}) {
    final group = image.format.group;
    final stepY = math.max(1, image.height ~/ 36);
    final stepX = math.max(1, image.width ~/ 64);

    int sum = 0, count = 0;

    if (group == ImageFormatGroup.bgra8888) {
      final p = image.planes[0];
      final bytes = p.bytes;
      final stride = p.bytesPerRow;

      for (int r = 0; r < image.height; r += stepY) {
        final rowStart = r * stride;
        for (int c = 0; c < image.width; c += stepX) {
          final idx = rowStart + c * 4;
          if (idx + 2 >= bytes.length) continue;
          final b = bytes[idx];
          final g = bytes[idx + 1];
          final r8 = bytes[idx + 2];
          final l = ((299 * r8 + 587 * g + 114 * b) / 1000).round();
          sum += l;
          count++;
        }
      }
    } else {
      final y = image.planes.first;
      final bytes = y.bytes;
      final stride = y.bytesPerRow;

      for (int r = 0; r < image.height; r += stepY) {
        final rowStart = r * stride;
        for (int c = 0; c < image.width; c += stepX) {
          final idx = rowStart + c;
          if (idx >= bytes.length) continue;
          sum += bytes[idx];
          count++;
        }
      }
    }

    final raw = (count == 0) ? 0.0 : (sum / count);
    if (!smooth) return raw;
    final alpha = 0.25;
    _lumaEma =
    (_lumaEma == null) ? raw : (_lumaEma! + alpha * (raw - _lumaEma!));
    return _lumaEma!.clamp(0.0, 255.0);
  }

  String _statusForLuma(double v) {
    if (v < 30) return "Very dark âŒ";
    if (v < 60) return "Too dim âŒ";
    if (v < 100) return "Dim light âš ï¸";
    if (v < 160) return "Good lighting âœ…";
    if (v < 220) return "Excellent lighting ğŸŒŸ";
    return "Too bright âš ï¸";
  }

  void _blendProgress(double target, {double smooth = 0.22}) {
    target = target.clamp(0.0, 1.0);
    final now = DateTime.now();
    final dt = (_lastBlendTs == null)
        ? 1.0 / 60.0
        : (now.difference(_lastBlendTs!).inMilliseconds / 1000.0)
        .clamp(0.0, 0.25);
    _lastBlendTs = now;

    const double deadzone = 0.004;
    if ((target - _ratioProgress).abs() < deadzone) {
      _setRatioProgress = target;
      return;
    }

    final s = smooth.clamp(0.0, 0.99);
    final alpha = 1 - math.pow(1 - s, dt * 60.0);

    double desired = _ratioProgress + (target - _ratioProgress) * alpha;

    const double maxUnitsPerSec = 1.2;
    final double maxStep = maxUnitsPerSec * dt;
    final double step = (desired - _ratioProgress);
    if (step.abs() > maxStep) {
      desired = _ratioProgress + step.sign * maxStep;
    }

    if (desired > 0.995) desired = 1.0;
    if (desired < 0.005) desired = 0.0;

    _setRatioProgress = desired;
  }

  void _collapseProgressFast({double factor = 0.25}) {
    final f = factor.clamp(0.05, 0.95);
    double next = _ratioProgress * (1.0 - f);
    if (next < 0.02) next = 0.0;
    _setRatioProgress = next;
  }

  // Public
  Future<void> tapNextEmployee() async {
    _stopCountdown(force: true);
    _isSnapshotting = false;

    // Ù†Ø¸Ù‘Ù Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    _livenessResult = null;
    _faceRecognitionResult = null;
    _attendanceResult = null;
    _capturedFile = null;
    _lastFaceRect = null;
    _sizeRaw = null;
    _ratioProgress = 0.0;
    _insideOval = false;
    _faceDetected = false;
    _setCaptureEligible(false);

    _showScreensaver = false;
    _cameraOpen = true;

    notifyListeners();

    // âœ… Ø¨Ø¯Ù„ Ù…Ø§ Ù†Ø¹ÙŠØ¯ init ÙƒÙ„ Ù…Ø±Ø©:
    if (_controller == null) {
      // Ø£ÙˆÙ„ Ù…Ø±Ø© ÙÙ‚Ø· Ø£Ùˆ Ù„Ùˆ ÙØ¹Ù„Ø§Ù‹ Ø§Ø®ØªÙØª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
      await _initCamera();
    } else if (_controller!.value.isInitialized) {
      // Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¬Ø§Ù‡Ø²Ø© â†’ Ø¨Ø³ Ø§Ø³ØªØ£Ù†Ù Ø§Ù„Ø¨Ø«
      await _startStreamSafely();
    } else {
      // Ø­Ø§Ù„Ø© Ù†Ø§Ø¯Ø±Ø©: Ø¹Ù†Ø¯Ù†Ø§ controller Ù„ÙƒÙ† Ù…Ùˆ Ù…Ù‡ÙŠØ£
      try {
        await _controller!.initialize();
        await _controller!.lockCaptureOrientation(DeviceOrientation.portraitUp);
        await _startStreamSafely();
      } catch (e) {
        // fallback Ù„Ùˆ ÙØ´Ù„ â†’ Ø¥Ø¹Ø§Ø¯Ø© init ÙƒØ§Ù…Ù„Ø©
        await _initCamera();
      }
    }
  }

  // ======= ğŸ’¡ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø­Ø¶ÙˆØ± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ ØªÙˆÙØ± employee_id   =======
  Future<void> _maybeAutoPostAttendance() async {
    if (_postedAttendanceForThisCapture) {
      debugPrint('[ATT] Skipped: already posted for this capture.');
      return;
    }

    final recog = _faceRecognitionResult;
    if (recog == null) {
      debugPrint('[ATT] Skipped: no faceRecognitionResult yet.');
      return;
    }

    // Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙƒÙ…Ø§ Ù‡ÙŠ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©
    debugPrint('[FR][RAW] $recog');


    // ==== Helpers Ù…Ø­Ù„ÙŠØ© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø£Ù…Ø§Ù† ====
    dynamic _get(Map m, List path) {
      dynamic cur = m;
      for (final k in path) {
        if (cur is Map && cur.containsKey(k)) {
          cur = cur[k];
        } else {
          return null;
        }
      }
      return cur;
    }

    int? _asInt(dynamic v) {
      if (v == null) return null;
      if (v is int) return v;
      if (v is String) return int.tryParse(v);
      if (v is num) return v.toInt();
      return null;
    }

    String? _asStr(dynamic v) {
      if (v == null) return null;
      if (v is String) return v.trim().isEmpty ? null : v.trim();
      if (v is num) return v.toString();
      return null;
    }

    // ==== Ø¬Ø±Ù‘Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ====
    final Map<String, dynamic> R = Map<String, dynamic>.from(recog);

    final employeeId = _asInt(
        _get(R, ['employee_id']) ??
            _get(R, ['match', 'employee_id']) ??
            _get(R, ['match', 'employee', 'id']) ??
            _get(R, ['match', 'employee_data', 'id'])
    );




    if (employeeId == null) {
      debugPrint('[ATT] Skipped: neither employee_id  found in recog.');
      return;
    }

    final nowStr = formatDateTime(DateTime.now());
    // final nowStr = formatDateTime(DateTime.parse("2026-02-12 12:55:00"));

    try {
      ApiResult result;
      if (employeeId == null) {
        debugPrint('[ATT] Skip: employee_id is null, not posting attendance.');
        return;
      }

      result = await AttendanceService.storeByEmployeeId(
        employeeId: employeeId,
        dateTime: nowStr,
      );


// Ø®Ø²Ù‘Ù† Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£ÙˆÙ„Ù‰ (Ù‚Ø¯ ØªØ­ØªÙˆÙŠ "please specify type")
      final String firstMessage = (result.message ?? '').toString().trim();

// âœ… Ù„Ùˆ Ø§Ù„Ø³ÙŠØ±ÙØ± Ø·Ù„Ø¨ type â†’ Ø§ÙØªØ­ Ø§Ù„Ù…ÙˆØ¯Ø§Ù„
      if (result.needType == true) {
        final String? picked = await (onRequireType?.call());

        if (picked != null) {
          // Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ø¹ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ø®ØªØ§Ø±
          result = await AttendanceService.storeByEmployeeId(
            employeeId: employeeId,
            dateTime: nowStr,
            type: picked,
          );
        } else {
          // Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£ØºÙ„Ù‚ Ø§Ù„Ù…ÙˆØ¯Ø§Ù„ â†’ Ø§Ø·Ø¨Ø¹ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ±ÙØ± ÙƒÙ…Ø§ Ù‡ÙŠ (Ø£Ùˆ Ø¨Ø¯ÙŠÙ„)
          _attendanceResult = {
            'status': 'error',
            'message': firstMessage.isNotEmpty ? firstMessage : 'Type not selected',
          };
          _postedAttendanceForThisCapture = true;
          _lastApiOk = false;
          _lastApiMessage = _attendanceResult!['message'];
          notifyListeners();
          return;
        }
      }

// Ù…Ù† Ù‡Ù†Ø§: Ø¹Ù†Ø¯Ù†Ø§ Ù†ØªÙŠØ¬Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© (Ù†Ø¬Ø§Ø­/ÙØ´Ù„) Ù…Ù† Ø£Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨ÙŠÙ†
      debugPrint('resultAttendance ${result.ok}__${result.message}');
      _postedAttendanceForThisCapture = true;

      _lastApiOk = result.ok;
      _lastApiMessage = result.message;

// Ø£Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ±ÙØ± ÙƒÙ…Ø§ Ù‡ÙŠ
      _attendanceResult = {
        'status': result.ok ? 'ok' : 'error',
        'message': result.message,
      };

      debugPrint('[ATT][RESPONSE] ok=${result.ok}, msg=${result.message}');
      notifyListeners();
      debugPrint('resultAttendance${result.ok}__${result.message}');
      _postedAttendanceForThisCapture = true;

      _lastApiOk = result.ok;
      _lastApiMessage = result.message;

      // âœ… Ø£Ø¶Ù Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±:
      _attendanceResult = {
        'status': result.ok ? 'ok' : 'error',
        'message': result.message,
      };

      debugPrint('[ATT][RESPONSE] ok=${result.ok}, msg=${result.message}');
      notifyListeners();
    } catch (e) {
      debugPrint('[ATT][ERROR] $e');
    }
  }
  Future<void> toggleCamera() async {
    final hasFront = _frontCamera != null;
    final hasRear  = _rearCamera  != null;

    if (!(hasFront && hasRear)) {
      debugPrint('â„¹ï¸ Only one camera available; toggle ignored.');
      return;
    }

    try {
      await _stopStreamSafely();
      await _disposeCamera();

      _useFront = !_useFront; // âœ… Ù‚Ù„Ø¨ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø¨ÙŠÙ† Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø®Ù„ÙÙŠØ©

      // Ù†Ø¸Ù‘Ù Ø§Ù„Ø­Ø§Ù„Ø©
      _capturedFile = null;
      _livenessResult = null;
      _faceRecognitionResult = null;
      _attendanceResult = null;
      _ratioProgress = 0.0;
      _insideOval = false;
      _faceDetected = false;
      _setCaptureEligible(false);
      notifyListeners();

      await _initCamera();   // âœ… Ø£Ø¹Ø¯ ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
      _resetInactivity();
    } catch (e) {
      debugPrint('âŒ toggleCamera error: $e');
    }
  }


}
