// =============================
// File: lib/features/face_liveness/controller/face_liveness_controller.dart
// =============================
import 'dart:async';
import 'dart:io';
import 'dart:math' as math;
import 'dart:typed_data';
import 'dart:ui' as ui;

import 'package:camera/camera.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:flutter/widgets.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:intl/intl.dart';

import '../constants.dart';
import '../services/network_service.dart';

class FaceLivenessController extends ChangeNotifier with WidgetsBindingObserver {
  // ===== Dependencies / Services =====
  final LivenessNetworkService _net = LivenessNetworkService();

  // ===== Camera =====
  CameraController? _controller;
  CameraController? get controller => _controller;
  CameraDescription? _frontCamera;

  // ===== Detector =====
  FaceDetector? _detector;
  bool _isDetecting = false;

  // ===== State (public getters) =====
  bool _cameraOpen = true;
  bool get cameraOpen => _cameraOpen;

  bool _faceDetected = false;
  bool get faceDetected => _faceDetected;

  double? _faceRatioValue;
  double? get faceRatioValue => _faceRatioValue;

  double _ratioProgress = 0.0;
  DateTime? _lastFaceTs;

  /// Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡Ø© Ù…Ù† Ø§Ù„Ø´Ø§Ø´Ø© (0..1)
  double get ratioProgress => _ratioProgress;

  bool _centeredInOval = false;
  bool get centeredInOval => _centeredInOval;

  /// 0..1 ÙƒÙ„Ù…Ø§ Ø§Ù‚ØªØ±Ø¨Øª Ù…Ù† Ø§Ù„Ù…Ø±ÙƒØ² Ø²Ø§Ø¯Øª Ø§Ù„Ù‚ÙŠÙ…Ø© (1.0 = ÙÙŠ Ù‚Ù„Ø¨ Ø§Ù„Ù…Ø±ÙƒØ²)
  double _centerScore = 0.0;
  double get centerScore => _centerScore;

  /// Ø¥Ø²Ø§Ø­Ø© Ù…Ø±ÙƒØ² Ø§Ù„ÙˆØ¬Ù‡ Ø¹Ù† Ù…Ø±ÙƒØ² Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ø¨Ø§Ù„Ø¨ÙŠÙƒØ³Ù„ (Ù„Ù„Ø¯ÙŠØ¨Øº/Ø§Ù„Ø¹Ø±Ø¶)
  Offset? _centerOffsetPx;
  Offset? get centerOffsetPx => _centerOffsetPx;

  /// ØªØ­Ø¯ÙŠØ« Ø¯Ø§Ø®Ù„ÙŠ
  set _setRatioProgress(double v) {
    _ratioProgress = v.clamp(0.0, 1.0);
    notifyListeners();
  }


  int _frameCount = 0;
  int _detectCounter = 0;

  double? _brightnessLevel; // 0..255
  double? get brightnessLevel => _brightnessLevel;
  String? _brightnessStatus;
  String? get brightnessStatus => _brightnessStatus;

  int? _countdown;
  int? get countdown => _countdown;

  bool get isCountdownActive => _countdown != null && _countdown! > 0;
  bool _isSnapshotting = false;

  Map<String, dynamic>? _livenessResult;
  Map<String, dynamic>? get livenessResult => _livenessResult;

  Map<String, dynamic>? _faceRecognitionResult;
  Map<String, dynamic>? get faceRecognitionResult => _faceRecognitionResult;

  XFile? _capturedFile;
  XFile? get capturedFile => _capturedFile;

  // For painters
  Rect? _lastFaceRect;
  Rect? get lastFaceRect => _lastFaceRect;

  Size? _latestImageSize;
  Size? get latestImageSize => _latestImageSize;

  bool _insideOval = false;
  bool get insideOval => _insideOval;

  bool _readyForNextImage = true;
  bool _isStreaming = false;
  bool _streamLock = false;

  // Screensaver / inactivity
  Timer? _inactivityTimer;
  Timer? _inactivityTicker;
  int? _screensaverCountdown;
  int? get screensaverCountdown => _screensaverCountdown;

  bool _showScreensaver = false;
  bool get showScreensaver => _showScreensaver;

  // Clock animation for screensaver
  final List<String> _clockPositions = ['center', 'right', 'left'];
  int _clockPosIndex = 0;
  int get clockPosIndex => _clockPosIndex;

  Timer? _clockMoveTimer;
  bool _clockBlink = false;
  bool get clockBlink => _clockBlink;

  Timer? _clockBlinkTimer;
  DateTime _now = DateTime.now();
  DateTime get now => _now;
  Timer? _nowTicker;

  // Layout-dependent
  Size _screenSize = Size.zero;
  set screenSize(Size s) => _screenSize = s;

  double _estimateLuma(CameraImage image) {
    // Ù†Ø®ØªØ§Ø± Ù‚Ù†Ø§Ø© Ù…Ø¶ÙŠØ¦Ø© Ø­Ø³Ø¨ ÙÙˆØ±Ù…Ø§Øª Ø§Ù„ØµÙˆØ±Ø©
    final group = image.format.group;

    // Ø®Ø·ÙˆØ§Øª Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ø®ÙÙŠÙØ©
    final stepY = math.max(1, image.height ~/ 36);
    final stepX = math.max(1, image.width  ~/ 64);

    int sum = 0, count = 0;

    if (group == ImageFormatGroup.bgra8888) {
      // iOS ØºØ§Ù„Ø¨Ø§Ù‹ â€” Plane ÙˆØ§Ø­Ø¯ BGRA (4 Ø¨Ø§ÙŠØª/Ø¨ÙƒØ³Ù„)
      final p = image.planes[0];
      final bytes = p.bytes;
      final stride = p.bytesPerRow; // 4*width Ø£Ùˆ Ø£ÙƒØ¨Ø±

      for (int r = 0; r < image.height; r += stepY) {
        final rowStart = r * stride;
        for (int c = 0; c < image.width; c += stepX) {
          final idx = rowStart + c * 4;
          final b = bytes[idx];
          final g = bytes[idx + 1];
          final r8 = bytes[idx + 2];
          // ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠØ© (BT.601)
          final luma = ((299 * r8 + 587 * g + 114 * b) / 1000).round(); // 0..255
          sum += luma; count++;
        }
      }
    } else {
      // Android NV21/YUV420 â€” Ø£ÙˆÙ„ plane Ù‡Ùˆ Y (Ø¥Ø¶Ø§Ø¡Ø© Ù…Ø¨Ø§Ø´Ø±Ø©)
      final yPlane = image.planes.first;
      final bytes = yPlane.bytes;
      final stride = yPlane.bytesPerRow;

      for (int r = 0; r < image.height; r += stepY) {
        final rowStart = r * stride;
        for (int c = 0; c < image.width; c += stepX) {
          sum += bytes[rowStart + c]; // Ù‚ÙŠÙ…Ø© Y Ø¬Ø§Ù‡Ø²Ø© 0..255
          count++;
        }
      }
    }

    if (count == 0) return 0.0;
    return sum / count; // 0..255
  }


  String _statusForLuma(double v) {
    if (v < 30)  return "Very dark âŒ";
    if (v < 60)  return "Too dim âŒ";
    if (v < 100) return "Dim light âš ï¸";
    if (v < 160) return "Good lighting âœ…";
    if (v < 220) return "Excellent lighting ğŸŒŸ";
    return "Too bright âš ï¸";
  }

  // Lifecycle
  Future<void> init() async {
    WidgetsBinding.instance.addObserver(this);
    _initDetector();
    _resetInactivity();
    _nowTicker = Timer.periodic(const Duration(seconds: 1), (_) {
      if (_showScreensaver) {
        _now = DateTime.now();
        notifyListeners();
      }
    });
    await _initCamera();
  }

  Future<void> disposeAll() async {
    WidgetsBinding.instance.removeObserver(this);
    _stopEverything();
    await _disposeCamera();
    _detector?.close();
    _detector = null;
    super.dispose();
  }

  // Handle app pause/resume
  @override
  void didChangeAppLifecycleState(AppLifecycleState state) async {
    if (_controller == null) return;
    if (state == AppLifecycleState.inactive || state == AppLifecycleState.paused) {
      await _stopStreamSafely();
    } else if (state == AppLifecycleState.resumed) {
      if (!_showScreensaver) await _startStreamSafely();
    }
  }

  // ===== Init parts =====
  void _initDetector() {
    _detector = FaceDetector(
      options: FaceDetectorOptions(
        performanceMode: FaceDetectorMode.fast,
        enableContours: false,
        enableLandmarks: false,
        enableClassification: false,
        enableTracking: false,
      ),
    );
  }

  Future<void> _initCamera() async {
    final cameras = await availableCameras();
    _frontCamera = cameras.firstWhere(
          (c) => c.lensDirection == CameraLensDirection.front,
      orElse: () => cameras.first,
    );
    _controller = CameraController(
      _frontCamera!,
      ResolutionPreset.medium,
      enableAudio: false,
      imageFormatGroup: ImageFormatGroup.nv21,
    );
    await _controller!.initialize();
    await _controller!.lockCaptureOrientation(DeviceOrientation.portraitUp);

    _latestImageSize = Size(
      _controller!.value.previewSize?.width ?? 1280,
      _controller!.value.previewSize?.height ?? 720,
    );

    _controller!.addListener(() async {
      final v = _controller!.value;
      if (v.hasError) {
        await _stopStreamSafely();
        await _startStreamSafely();
      }
    });

    await _startStreamSafely();
    notifyListeners();
  }

  Future<void> _disposeCamera() async {
    try {
      if (_controller != null) {
        await _stopStreamSafely();
        await _controller!.dispose();
      }
    } catch (_) {}
    _controller = null;
  }

  // ===== Inactivity / screensaver =====
  void _resetInactivity() {
    _inactivityTimer?.cancel();
    _inactivityTicker?.cancel();
    _screensaverCountdown = kScreensaverSeconds;

    _inactivityTicker = Timer.periodic(const Duration(seconds: 1), (_) {
      if (!_showScreensaver) {
        if (_screensaverCountdown != null && _screensaverCountdown! > 0) {
          _screensaverCountdown = _screensaverCountdown! - 1;
          notifyListeners();
        }
      }
    });

    _inactivityTimer = Timer(Duration(seconds: kScreensaverSeconds), () async {
      _showScreensaver = true;
      _cameraOpen = false;
      notifyListeners();
      await _disposeCamera();
      _startClockMover();
    });

    _screensaverCountdown = kScreensaverSeconds;
    notifyListeners();
  }

  void userActivity() {
    if (_showScreensaver) return; // ØªØ¬Ø§Ù‡Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø´Ø§Ø´Ø© Ø§Ù„ØªÙˆÙ‚Ù Ø­ØªÙ‰ Ù†Ù‚Ø± Ø§Ù„Ø¹ÙˆØ¯Ø©
    _resetInactivity();
  }

  void _startClockMover() {
    _clockMoveTimer?.cancel();
    _clockMoveTimer = Timer.periodic(const Duration(milliseconds: kClockDwellMs), (_) {
      _clockBlink = true;
      notifyListeners();
      _clockBlinkTimer?.cancel();
      _clockBlinkTimer = Timer(const Duration(milliseconds: 70), () {
        _clockPosIndex = (_clockPosIndex + 1) % _clockPositions.length;
        _clockBlink = false;
        notifyListeners();
      });
    });
  }

  Future<void> exitScreensaverAndReopen() async {
    _showScreensaver = false;
    _cameraOpen = true;
    _capturedFile = null;
    _livenessResult = null;
    _faceRecognitionResult = null;
    _stopCountdown(force: true);
    _isSnapshotting = false;
    _isDetecting = false;
    _readyForNextImage = true;
    _faceDetected = false;
    _faceRatioValue = null;
    _ratioProgress = 0.0;
    _brightnessLevel = null;
    _brightnessStatus = null;
    _detectCounter = 0;
    _lastFaceRect = null;
    _insideOval = false;

    await _initCamera();
    _resetInactivity();
    notifyListeners();
  }

  // ===== Stream control =====
  Future<void> _startStreamSafely() async {
    if (_controller == null) return;
    if (_streamLock) return;
    if (_controller!.value.isStreamingImages || _isStreaming) return;
    if (!_controller!.value.isInitialized) return;
    try {
      _streamLock = true;
      await _controller!.startImageStream(_onNewCameraImage);
      _isStreaming = true;
    } catch (_) {
    } finally {
      _streamLock = false;
    }
  }

  Future<void> _stopStreamSafely() async {
    if (_controller == null) return;
    if (_streamLock) return;
    if (!_controller!.value.isStreamingImages && !_isStreaming) return;
    try {
      _streamLock = true;
      await _controller!.stopImageStream();
    } catch (_) {
    } finally {
      _isStreaming = false;
      _streamLock = false;
    }
  }

  Future<void> _resumeLivePreview() async {
    _capturedFile = null;
    _livenessResult = null;
    _faceRecognitionResult = null;
    _cameraOpen = true;
    _isSnapshotting = false;
    _readyForNextImage = true;
    _insideOval = false;
    _ratioProgress = 0.0;
    notifyListeners();
    await Future.delayed(const Duration(milliseconds: 120));
    await _startStreamSafely();
  }

  // ===== Core vision loop =====

  Future<void> _onNewCameraImage(CameraImage image) async {
    // Ø­Ù…Ø§ÙŠØ© ØªØ¯ÙÙ‚ Ø§Ù„ØµÙˆØ±
    if (!_readyForNextImage || !_cameraOpen) return;
    _readyForNextImage = false;

    // ØªØ­Ø¯ÙŠØ« Ø¢Ø®Ø± Ø­Ø¬Ù… Ø®Ø§Ù… Ù„Ù„ØµÙˆØ±Ø©
    _latestImageSize = Size(image.width.toDouble(), image.height.toDouble());

    // ===== Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙƒÙ„ N Ø¥Ø·Ø§Ø± =====
    _frameCount = (_frameCount + 1) % kBrightnessSampleEveryN;
    if (_frameCount == 0) {
      final luma = _estimateLuma(image); // 0..255
      _brightnessLevel = luma;
      _brightnessStatus = _statusForLuma(luma);
      debugPrint('LUMA=${_brightnessLevel?.toStringAsFixed(1)} | ${_brightnessStatus}');
      notifyListeners(); // Ù…Ù‡Ù… Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø´Ø±ÙŠØ­Ø© Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙÙˆØ±Ù‹Ø§
    }

    // ===== ØªÙ‚Ù„ÙŠÙ„ ÙƒÙ„ÙØ© Ø§Ù„ÙƒØ´Ù (decimation) =====
    _detectCounter = (_detectCounter + 1) % kDetectEveryN;
    if (_detectCounter != 0) {
      _readyForNextImage = true;
      return;
    }
    if (_isDetecting) {
      _readyForNextImage = true;
      return;
    }
    _isDetecting = true;

    try {
      // ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…ÙƒØªØ¨Ø©
      final inputImage = _toInputImage(image);
      final faces = await _detector!.processImage(inputImage);

      if (faces.isNotEmpty) {
        // Ø§Ø®ØªØ± Ø£ÙƒØ¨Ø± ÙˆØ¬Ù‡ (ØºØ§Ù„Ø¨Ù‹Ø§ Ø§Ù„Ø£Ù‚Ø±Ø¨)
        final face = faces.reduce((a, b) =>
        (a.boundingBox.width * a.boundingBox.height) >
            (b.boundingBox.width * b.boundingBox.height) ? a : b);

        final rect = face.boundingBox; // Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª portrait
        _lastFaceRect = rect;

        final rawW = image.width.toDouble();
        final rawH = image.height.toDouble();

        // --- Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø´Ø§Ø´Ø© ---
        final bool isFront =
            _frontCamera?.lensDirection == CameraLensDirection.front;

        // 1) Ù‡Ù„ Ø§Ù„ÙˆØ¬Ù‡ Ø¨ÙƒØ§Ù…Ù„Ù‡ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠØŸ
        _insideOval = _isFaceInsideOvalOnScreen(
          faceCenter: rect.center,
          imageRawSize: Size(rawW, rawH),
          screenSize: _screenSize,
          isFront: isFront,
        );

        // 2) Ù‡Ù„ Ø§Ù„ÙˆØ¬Ù‡ Ù…ØªÙ…Ø±ÙƒØ² Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ ÙÙŠ Ù‚Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠØŸ (Ù„Ù„Ø¥Ø¸Ù‡Ø§Ø±/Ø§Ù„ØªØ´ÙˆÙŠÙ‚ Ø¥Ù† Ø§Ø­ØªØ¬Øª)
        _centeredInOval = _isFaceCenteredInOvalOnScreen(
          faceCenter: rect.center,
          imageRawSize: Size(rawW, rawH),
          screenSize: _screenSize,
          isFront: isFront,
        );

        // 3) Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªÙ‚Ø¯Ù‘Ù…: Ø§Ù„Ø­Ø¬Ù… (0..1) ÙˆØ§Ù„ØªÙ…Ø±ÙƒØ² (0..1)
        final double sizeFactor = _sizeScore(rect, Size(rawW, rawH)); // 0..1
        final double posFactor = _positionFactor(
          faceCenterRaw: rect.center,
          imageRawSize: Size(rawW, rawH),
          screenSize: _screenSize,
          isFront: isFront,
        ); // 0..1 (0 Ø¥Ø°Ø§ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ)

        // 4) Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø±ÙƒÙ‘Ø¨ Ù„Ø´Ø±ÙŠØ· Face Fit
        if (!_insideOval || posFactor == 0.0) {
          // Ø®Ø±Ø¬ Ø§Ù„ÙˆØ¬Ù‡ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ø£Ùˆ ÙŠÙƒØ§Ø¯: ØµÙÙ‘Ø± Ø³Ø±ÙŠØ¹Ù‹Ø§ (Ø¥Ø­Ø³Ø§Ø³ Ø­Ø§Ø³Ù… ÙˆÙˆØ§Ø¶Ø­)
          _setRatioProgress = 0.0;
          notifyListeners();
        } else {
          // Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ: Ø§Ù…Ø²Ø¬ Ø§Ù„Ø­Ø¬Ù… Ù…Ø¹ Ø§Ù„ØªÙ…Ø±ÙƒØ²
          // ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØ²Ù† Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ù„Ùˆ Ø£Ø±Ø¯Øª (Ù…Ø«Ù„Ø§Ù‹ 0.7 * size + 0.3 * pos)
          final double targetProgress = (sizeFactor * posFactor).clamp(0.0, 1.0);
          _blendProgress(targetProgress, smooth: 0.22);
        }

        // 5) ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© "ÙˆØ¬Ù‡ Ù…ÙÙƒØªØ´Ù" (ÙŠÙ…ÙƒÙ† Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø© Ø¨Ø­Ø³Ø¨ ØªØ¬Ø±Ø¨ØªÙƒ)
        _updateFaceDetected(sizeFactor >= (kMinFaceRatio * 0.6));

        // 6) Ø§Ù„ØªØ­ÙƒÙ‘Ù… Ø¨Ø§Ù„Ø¹Ø¯Ù‘Ø§Ø¯ (Countdown) Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙˆØ·
        if (_faceDetected && _insideOval /*&& _ratioProgress >= 0.99*/) {
          _beginCountdown();
        } else {
          _stopCountdown();
        }
      } else {
        // Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ¬ÙˆÙ‡: Ù‡Ø¨ÙˆØ· Ø³Ø±ÙŠØ¹ Ù„Ù„Ø´Ø±ÙŠØ· ÙˆØ¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ø¯Ù‘Ø§Ø¯
        _lastFaceRect = null;
        _insideOval = false;
        _updateFaceDetected(false);
        _collapseProgressFast(factor: 0.35); // ØµÙØ± Ø³Ø±ÙŠØ¹Ù‹Ø§
        _stopCountdown();
      }
    } catch (_) {
      // ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù„Ø­Ø¸ÙŠØ©
    } finally {
      _isDetecting = false;
      _readyForNextImage = true;
      notifyListeners();
      // Ù…Ù‡Ù„Ø© ØµØºÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ù„Ù…Ù†Ø¹ ØªØ´Ø¨Ù‘Ø¹ Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¤ÙŠØ©
      await Future.delayed(const Duration(milliseconds: 2));
    }
  }

  bool _isFaceCenteredInOvalOnScreen({
    required Offset faceCenter,
    required Size imageRawSize,
    required Size screenSize,
    required bool? isFront,
    double epsilonPct = kCenterEpsilonPct, // ØªÙ‚Ø¨Ù‘Ù„ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
  }) {
    if (screenSize == Size.zero) {
      _centerScore = 0.0;
      _centerOffsetPx = null;
      return false;
    }

    // Ù†ÙØ³ Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© (portrait)
    final srcW = imageRawSize.height; // portrait width
    final srcH = imageRawSize.width;  // portrait height
    final scale = math.max(screenSize.width / srcW, screenSize.height / srcH);
    final dxPad = (screenSize.width  - srcW * scale) / 2.0;
    final dyPad = (screenSize.height - srcH * scale) / 2.0;

    double cx = faceCenter.dx * scale + dxPad;
    final double cy = faceCenter.dy * scale + dyPad;

    // Ù…Ø±Ø¢Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    if (isFront == true) {
      final midX = screenSize.width / 2;
      cx = 2 * midX - cx;
    }

    // Ù…Ø±ÙƒØ² ÙˆÙ†ØµÙÙŠ Ù‚Ø·Ø± Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©
    final ovalCx = screenSize.width  * (0.5 + kOvalCxOffsetPct);
    final ovalCy = screenSize.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = (screenSize.width  * kOvalRxPct);
    final ovalRy = (screenSize.height * kOvalRyPct);

    // Ø¥Ø²Ø§Ø­Ø© Ø§Ù„ÙˆØ¬Ù‡ Ø¹Ù† Ø§Ù„Ù…Ø±ÙƒØ² (Ø¨ÙŠÙƒØ³Ù„)
    final offXpx = cx - ovalCx;
    final offYpx = cy - ovalCy;
    _centerOffsetPx = Offset(offXpx, offYpx);

    // Ø·Ø¨ÙŠØ¹ (normalize) Ø§Ù„Ø¥Ø²Ø§Ø­Ø© Ø¹Ù„Ù‰ Ø£Ù†ØµØ§Ù Ø§Ù„Ø£Ù‚Ø·Ø§Ø±
    final dxn = ovalRx == 0 ? 0.0 : offXpx / ovalRx; // Ù†Ø³Ø¨Ø© -1..1
    final dyn = ovalRy == 0 ? 0.0 : offYpx / ovalRy;

    // Ù†ØµÙ Ù‚Ø·Ø± â€œÙ…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø±ÙƒØ²â€ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§ ÙƒÙ†Ø³Ø¨Ø© Ù…Ù† Ù†ØµÙ Ø§Ù„Ù‚Ø·Ø± Ø§Ù„Ø£ØµÙ„ÙŠ
    final rAllow = epsilonPct.clamp(0.02, 0.9);

    // Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø±ÙƒØ² Ø¯Ø§Ø®Ù„ Ø¬Ù‡Ø§Ø² Ø¥Ø­Ø¯Ø§Ø«ÙŠ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
    final r = math.sqrt(dxn * dxn + dyn * dyn); // 0 Ø¹Ù†Ø¯ Ø§Ù„Ù‚Ù„Ø¨

    // Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø­Ø§Ø°Ø§Ø©: 1 Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø±ÙƒØ²ØŒ 0 Ø¹Ù†Ø¯ Ø­Ø¯ rAllow Ø£Ùˆ Ø®Ø§Ø±Ø¬Ù‡
    _centerScore = (1.0 - (r / rAllow)).clamp(0.0, 1.0);

    // true Ø¥Ø°Ø§ Ø¯Ø§Ø®Ù„ â€œÙ…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø±ÙƒØ²â€
    return r <= rAllow;
  }


  bool _isFaceInsideOvalOnScreen({
    required Offset faceCenter,
    required Size imageRawSize,
    required Size screenSize,
    required bool? isFront,
  }) {
    if (screenSize == Size.zero) return false;

    final srcW = imageRawSize.height; // portrait width
    final srcH = imageRawSize.width;  // portrait height
    final scale = math.max(screenSize.width / srcW, screenSize.height / srcH);
    final dx = (screenSize.width - srcW * scale) / 2.0;
    final dy = (screenSize.height - srcH * scale) / 2.0;

    final faceRect = _lastFaceRect;
    if (faceRect == null) return false;

    List<Offset> corners = [
      faceRect.topLeft,
      faceRect.topRight,
      faceRect.bottomLeft,
      faceRect.bottomRight,
    ];

    final ovalCx = screenSize.width * (0.5 + kOvalCxOffsetPct);
    final ovalCy = screenSize.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = (screenSize.width * kOvalRxPct) * kOvalInsideEpsilon;
    final ovalRy = (screenSize.height * kOvalRyPct) * kOvalInsideEpsilon;

    for (var point in corners) {
      double cx = point.dx * scale + dx;
      double cy = point.dy * scale + dy;

      // Ø¥Ø°Ø§ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø£Ù…Ø§Ù…ÙŠØ© Ù†Ø¹ÙƒØ³ X
      if (isFront == true) {
        final midX = screenSize.width / 2;
        cx = 2 * midX - cx;
      }

      const double relax = 0.80;

      final dxn = (cx - ovalCx) / (ovalRx / relax);
      final dyn = (cy - ovalCy) / (ovalRy / relax);
      final distance = dxn * dxn + dyn * dyn;

      if (distance > 1.0) return false; // Ù†Ù‚Ø·Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
    }

    return true; // Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø²ÙˆØ§ÙŠØ§ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
  }


  void _updateFaceDetected(bool detected) {
    if (detected == _faceDetected) return;
    _faceDetected = detected;
    if (!detected) {
      _ratioProgress = ui.lerpDouble(_ratioProgress, 0.0, 0.12)!;
    } else {
      if (!_isSnapshotting) {
        _livenessResult = null;
        _faceRecognitionResult = null;
        _capturedFile = null;
      }
      _resetInactivity();
    }
  }

  void _beginCountdown() {
    if (_isSnapshotting || !_cameraOpen) return;
    if (isCountdownActive) return;
    _countdown = kCountdownSeconds;

    userActivity();
    Timer.periodic(const Duration(seconds: 1), (Timer t) async {
      if (_showScreensaver) { t.cancel(); return; }
      if (!_insideOval || !_faceDetected) { _stopCountdown(); t.cancel(); return; }
      if (_isSnapshotting) { t.cancel(); return; }

      if (_countdown != null && _countdown! > 0) {
        _countdown = _countdown! - 1;
        notifyListeners();
        if (_countdown == 0) {
          _countdown = 0;
          _isSnapshotting = true;
          notifyListeners();
          t.cancel();
          scheduleMicrotask(() async { await _handleLivenessCheck(); });
        }
      }
    });
    notifyListeners();
  }

  void _stopCountdown({bool force = false}) {
    if (_isSnapshotting && !force) return;
    _countdown = null;
    notifyListeners();
  }

  // ===== Capture & backend =====
  Future<void> _handleLivenessCheck() async {
    if (!_insideOval || !_faceDetected) {
      _isSnapshotting = false;
      _readyForNextImage = true;
      _stopCountdown(force: true);
      await _startStreamSafely();
      return;
    }

    if (_controller == null || !_controller!.value.isInitialized) return;
    try {
      _readyForNextImage = false;
      _isDetecting = false;
      await _stopStreamSafely();
      await Future.delayed(const Duration(milliseconds: 80));

      if (!_insideOval || !_faceDetected) {
        _isSnapshotting = false;
        _readyForNextImage = true;
        _stopCountdown(force: true);
        await _startStreamSafely();
        return;
      }

      final file = await _controller!.takePicture();
      _capturedFile = file;
      _lastFaceRect = null;
      notifyListeners();

      if (kEnableLiveness) {
        final liveJson = await _net.sendLiveness(file.path);
        _livenessResult = liveJson ?? {'error': 'Invalid response'};
        notifyListeners();
      }
      if (kEnableFaceRecognition) {
        final recog = await _net.sendFaceRecognition(file.path);
        print('recoRequest{$recog}');
        _faceRecognitionResult = recog ?? {'error': 'Invalid response'};
        notifyListeners();
      }
      Timer(const Duration(milliseconds: kDisplayImageMs), () async {
        if (_showScreensaver) return;
        await _resumeLivePreview();
      });
    } catch (_) {
      _livenessResult = {'error': 'Error sending image to backend!'};
      notifyListeners();
    } finally {
      _isSnapshotting = false;
      _readyForNextImage = true;
    }
  }

  // ===== Helpers =====
  InputImage _toInputImage(CameraImage image) {
    final bytes = _concatenatePlanes(image.planes);
    final rotation = _rotationIntToImageRotation(
      _controller?.description.sensorOrientation ?? 0,
    );
    final inputFormat = (image.format.group == ImageFormatGroup.nv21)
        ? InputImageFormat.nv21
        : InputImageFormat.yuv420;

    return InputImage.fromBytes(
      bytes: bytes,
      metadata: InputImageMetadata(
        size: Size(image.width.toDouble(), image.height.toDouble()),
        rotation: rotation,
        format: inputFormat,
        bytesPerRow: image.planes.first.bytesPerRow,
      ),
    );
  }

  Uint8List _concatenatePlanes(List<Plane> planes) {
    final WriteBuffer allBytes = WriteBuffer();
    for (final Plane plane in planes) {
      allBytes.putUint8List(plane.bytes);
    }
    return allBytes.done().buffer.asUint8List();
  }

  InputImageRotation _rotationIntToImageRotation(int rotation) {
    switch (rotation) {
      case 0:
        return InputImageRotation.rotation0deg;
      case 90:
        return InputImageRotation.rotation90deg;
      case 180:
        return InputImageRotation.rotation180deg;
      case 270:
        return InputImageRotation.rotation270deg;
      default:
        return InputImageRotation.rotation0deg;
    }
  }

  void _stopEverything() {
    _inactivityTimer?.cancel();
    _inactivityTicker?.cancel();
    _clockMoveTimer?.cancel();
    _clockBlinkTimer?.cancel();
    _nowTicker?.cancel();
  }

  void _decayRatio() {
    // Ø¥Ù† Ù…Ø§ ÙÙŠ ÙˆØ¬Ù‡ØŒ Ø®ÙÙ‘Ø¶ Ø§Ù„Ù…Ø¤Ø´Ø± ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ Ù†Ø­Ùˆ Ø§Ù„ØµÙØ±
    const decay = 0.85;
    _setRatioProgress = _ratioProgress * decay;

    if (_ratioProgress < 0.005) {
      _setRatioProgress = 0.0;
    }
  }
  void _updateRatio(Rect faceRect, Size imageSize) {
    // âœ… Ù†Ù‚ÙŠØ³ â€œÙ‚ÙØ·Ø±â€ Ø§Ù„ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø£Ù‚ØµØ± Ø¨ÙØ¹Ø¯ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© (Ù…Ù‚ÙŠØ§Ø³ Ù„Ø§ ÙŠØªØ£Ø«Ø± Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡)
    final imgShort = math.min(imageSize.width, imageSize.height);
    final faceShort = math.min(faceRect.width, faceRect.height);

    // ÙƒÙ„Ù…Ø§ Ø§Ù‚ØªØ±Ø¨ Ø§Ù„ÙˆØ¬Ù‡ ÙŠÙƒØ¨Ø± faceShort â‡’ ØªØ²ÙŠØ¯ Ø§Ù„Ù†Ø³Ø¨Ø©
    final raw = (faceShort / imgShort).clamp(0.0, 1.0);

    // ğŸ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø°ÙŠ Ù†Ø¹ØªØ¨Ø±Ù‡ "Ù…Ù…ØªØ§Ø²" Ù„Ù„Ø§Ù„ØªÙ‚Ø§Ø· (Ø§Ø¶Ø¨Ø·Ù‡ Ø­Ø³Ø¨ ØªØµÙ…ÙŠÙ…Ùƒ/Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ)
    const target = 0.22; // Ø¬Ø±Ù‘Ø¨ Ø¨ÙŠÙ† 0.18 ~ 0.26

    // Ø­ÙˆÙ‘Ù„ Ø¥Ù„Ù‰ 0..1 (Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ù‡Ø¯Ù ÙŠÙÙ‚Øµ Ù„Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ 1.0)
    final targetProgress = (raw / target).clamp(0.0, 1.0);

    // ğŸ«§ ØªÙ†Ø¹ÙŠÙ… Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± (0.15..0.30 Ø­Ø³Ø¨ Ø°ÙˆÙ‚Ùƒ)
    const smooth = 0.22;
    _setRatioProgress = _ratioProgress + (targetProgress - _ratioProgress) * smooth;

    notifyListeners();
  }

  void _onFacesDetected(List<Face> faces, Size imageSize) {
    if (faces.isEmpty) {
      // Ù„Ùˆ Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØ¬Ù‡: Ù‚Ù„Ù„ Ø§Ù„Ù…Ø¤Ø´Ø± ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ Ù†Ø­Ùˆ Ø§Ù„ØµÙØ±
      _lastFaceTs = null;
      _decayRatio();
      return;
    }

    // Ø®Ø° Ø£ÙƒØ¨Ø± ÙˆØ¬Ù‡ (Ø£Ù‚Ø±Ø¨ ÙˆØ§Ø­Ø¯ Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¹Ø§Ø¯Ø©Ù‹)
    final face = faces.reduce((a, b) =>
    (a.boundingBox.width * a.boundingBox.height) >
        (b.boundingBox.width * b.boundingBox.height) ? a : b);

    _lastFaceTs = DateTime.now();
    _updateRatio(face.boundingBox, imageSize);
  }

  // ===== Progress helpers (size + position) =====

  /// ÙŠØ­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø¬Ù… 0..1 Ù…Ù† ØºÙŠØ± Ù…Ø§ ØªØªØ£Ø«Ø± Ø¨Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØµÙˆØ±Ø©
  double _sizeScore(Rect faceRect, Size imageSize, {double target = 0.22}) {
    final imgShort = math.min(imageSize.width, imageSize.height);
    final faceShort = math.min(faceRect.width, faceRect.height);
    final raw = (faceShort / (imgShort == 0 ? 1 : imgShort)).clamp(0.0, 1.0);
    // Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø°ÙŠ Ù†Ø¹ØªØ¨Ø±Ù‡ Ù…Ù…ØªØ§Ø² Ù„Ù„Ø§Ù„ØªÙ‚Ø§Ø· (Ø§Ø¶Ø¨Ø· target Ø­Ø³Ø¨ ØªØµÙ…ÙŠÙ…Ùƒ)
    return (raw / target).clamp(0.0, 1.0);
  }

  /// ÙŠØ­Ø³Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„ØªÙ…Ø±ÙƒØ² 0..1 Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ.
  /// 1 ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ²ØŒ ÙŠÙ‚Ù„ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ Ù†Ø­Ùˆ Ø§Ù„Ø­ÙˆØ§ÙØŒ 0 Ø¥Ø°Ø§ Ø®Ø±Ø¬ (distance>=1).
  double _positionFactor({
    required Offset faceCenterRaw,   // Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù…Ù† ÙØ¶Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø© (portrait)
    required Size imageRawSize,
    required Size screenSize,
    required bool isFront,
  }) {
    if (screenSize == Size.zero) return 0.0;

    // Ø¥Ø³Ù‚Ø§Ø· Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© (portrait)
    final srcW = imageRawSize.height; // portrait width
    final srcH = imageRawSize.width;  // portrait height
    final scale = math.max(screenSize.width / srcW, screenSize.height / srcH);
    final dxPad = (screenSize.width  - srcW * scale) / 2.0;
    final dyPad = (screenSize.height - srcH * scale) / 2.0;

    double cx = faceCenterRaw.dx * scale + dxPad;
    final double cy = faceCenterRaw.dy * scale + dyPad;

    // Ù…Ø±Ø¢Ø© Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    if (isFront) {
      final midX = screenSize.width / 2;
      cx = 2 * midX - cx;
    }

    // Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©
    final ovalCx = screenSize.width  * (0.5 + kOvalCxOffsetPct);
    final ovalCy = screenSize.height * (0.5 + kOvalCyOffsetPct);
    final ovalRx = (screenSize.width  * kOvalRxPct);
    final ovalRy = (screenSize.height * kOvalRyPct);

    // Ù…Ø³Ø§ÙØ© â€œÙ…ÙˆØ­Ø¯Ø©â€ Ù…Ù† Ù…Ø±ÙƒØ² Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
    final dxn = (cx - ovalCx) / (ovalRx == 0 ? 1 : ovalRx);
    final dyn = (cy - ovalCy) / (ovalRy == 0 ? 1 : ovalRy);
    final r = math.sqrt(dxn * dxn + dyn * dyn);

    if (r >= 1.0) return 0.0; // Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ

    // Ù…Ù†Ø­Ù†Ù‰ Ù†Ø§Ø¹Ù…: Ù‚Ø±ÙŠØ¨ Ù…Ù† 1 ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ²ØŒ ÙˆÙŠÙ‡Ø¨Ø· ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ Ù†Ø­Ùˆ Ø§Ù„Ø­Ø§ÙØ©
    // Ø§Ø¶Ø¨Ø· p Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ù…Ù†Ø­Ù†Ù‰ Ø£Ø¯Ù‚/Ø£ÙƒØ«Ø± Ø­Ø¯Ø©.
    const p = 1.4;
    return math.pow((1.0 - r), p).toDouble().clamp(0.0, 1.0);
  }

  /// ÙŠÙ…Ø²Ø¬ Ø§Ù„ØªÙ‚Ø¯Ù‘Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ù‡Ø¯Ù Ø¨Ø³Ù„Ø§Ø³Ø©
  void _blendProgress(double target, {double smooth = 0.22}) {
    _setRatioProgress = _ratioProgress + (target - _ratioProgress) * smooth;
    notifyListeners();
  }

  /// Ø®ÙØ¶ Ø³Ø±ÙŠØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ± Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ®ØªÙÙŠ Ø§Ù„ÙˆØ¬Ù‡ Ø£Ùˆ ÙŠØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¶Ø§ÙˆÙŠ
  void _collapseProgressFast({double factor = 0.15}) {
    _setRatioProgress = _ratioProgress * (1.0 - factor);
    if (_ratioProgress < 0.02) _setRatioProgress = 0.0;
    notifyListeners();
  }


  // Public actions
  Future<void> tapNextEmployee() async => _resumeLivePreview();
}
